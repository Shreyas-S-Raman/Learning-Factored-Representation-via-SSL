policy_head:
  algorithm: "PPO"  # Options: DQN, PPO, A2C
  num_models: 3
  num_parallel_envs: 4 #NOTE: log freqs have to be divisible by num_parallel_envs 
  train_interval: 2000000 #2000000, 4000000
  num_eval_eps: 50
  verbose: true
  video_length: 500
  video_log_freq: 20000
  reward_log_freq: 1000
  save_weight_freq: 50000
  wandb_log: false
  normalize: true

  learning_head: 'ssl-mask-reconst' #options [direct, supervised, ssl-mask-reconst, ssl-cov-ik,  ssl-cov, ssl-mask, dreamerv2] etc...

  #configs only for supervised and ssl methods
  vector_size_per_factor: 2
  num_factors: 15

  ppo_policy_kwargs:
    pi_dims: [64, 128]
    vf_dims: [64, 128]
    backbone_dim: 256
  ppo:
    # n_steps: 2048
    # n_epochs: 10
    # batch_size: 512
    # learning_rate: 3.0e-4
    # gamma: 0.99
    # gae_lambda: 0.9
    # clip_range: 0.2
    # ent_coef: 0.0005 #(or try 0.0001 if needed)
    # vf_coef: 0.5
    # max_grad_norm: 0.5

    # # n_steps: 256
    # n_epochs: 5
    # learning_rate: 2.5e-4
    # batch_size: 256
    # gamma: 0.99
    # gae_lambda: 0.95
    # clip_range: 0.2
    # # clip_range_vf: 0.65
    # # normalize_advantage: True
    ent_coef: 0.02
    # vf_coef: 0.3
    max_grad_norm: 0.4
    target_kl: null
    verbose: null
    seed: null

  dqn:
    buffer_size: 500000
    learning_starts: 10
    batch_size: 32
    learning_rate: 0.000005
    gradient_steps: 16
    tau: 0.8
    gamma: 0.99
    train_freq: 1
    exploration_fraction: 0.1
    exploration_final_eps: 0.1
    #max_grad_norm: 10
    seed: null

  a2c:
    learning_rate: 0.0003
    n_steps: 5
    gamma: 0.99
    ent_coef: 0.01
    vf_coef: 0.5
    max_grad_norm: 0.5
    rms_prop_eps: 1e-5
    gae_lambda: 0.95
    normalize_advantage: True
    seed: null